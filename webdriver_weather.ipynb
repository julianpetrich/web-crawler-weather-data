{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping for Energy Company\n",
    "\n",
    "The client is developing a market analysis platform for its Energy trading business. The code below shows a slighty modified version of a webcrawler that is scraping certain pages for data that may be used for further market analysis.\n",
    "\n",
    "In its original version the webcrawler would be included inside the ETL tool Talend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper():\n",
    "    \"\"\"Scrapes the temperature and wind speed data from website for\n",
    "        specific weather stations and writes the data into required JSON\n",
    "        file.\n",
    "    \n",
    "    :param station_dict: Dictionary that defines with stations to be scraped\n",
    "        and which data should be scraped for each corresponding station.\n",
    "        \n",
    "    :ivar data_temp: DataFrame for temperature data in the required format\n",
    "        ['Time', 'Value', 'Station'] in which Time represents the first day\n",
    "        of the month that was scraped, Value represents the average \n",
    "        temperature for that month, and Station represents the name of the \n",
    "        weather station\n",
    "    :ivar data_wind: DataFrame for wind speed data in the required format\n",
    "        ['Time', 'Value', 'Station'] in which Time represents the first day\n",
    "        of the month that was scraped, Value represents the average \n",
    "        wind speed for that month, and Station represents the name of the \n",
    "        weather station\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, station_dict):\n",
    "        \n",
    "        self.station_dict = station_dict\n",
    "        \n",
    "        self.data_temp = pd.DataFrame(columns=['Time', 'Value', 'Station'])\n",
    "        self.data_wind = pd.DataFrame(columns=['Time', 'Value', 'Station'])\n",
    "            \n",
    "        self.pipeline()\n",
    "            \n",
    "    def pipeline(self):\n",
    "        \"\"\"Starts the pipeline for the webscraper:\n",
    "            \n",
    "            1. Setting up the webdriver\n",
    "            2. Scraping the data from the webpage(s)\n",
    "            3. Writing the data into the required JSON format\n",
    "        \"\"\"\n",
    "        \n",
    "        self.set_webdriver()\n",
    "        self.get_data()\n",
    "        self.driver.close()\n",
    "        \n",
    "        self.write_data()\n",
    "                \n",
    "    def set_webdriver(self):\n",
    "        \"\"\"Sets up the webscraper:\n",
    "            \n",
    "            1. Sets the download folder to be same path\n",
    "            2. Sets Chromium to be used in headless mode\n",
    "            3. Sets executable path for Chromium\n",
    "        \"\"\" \n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        \n",
    "        # Sets download path\n",
    "        dir_current = os.getcwd()\n",
    "        prefs = {'download.default_directory': dir_current}\n",
    "        options.add_experimental_option('prefs', prefs)\n",
    "        \n",
    "        # Sets headless mode\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--window-size=1920x1080')\n",
    "        \n",
    "        # Sets executable path for Chromium Driver\n",
    "        exec_path = '/usr/local/bin/chromedriver'\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=options,\n",
    "                                       executable_path=exec_path)\n",
    "    \n",
    "    def load_data(self, driver, station, data_type):\n",
    "        \"\"\"Scrapes the required data from the webpage. Loads the data into\n",
    "        respective DataFrame (temperature or wind speed).\n",
    "            \n",
    "        :param driver: Webdriver that is used for scraping\n",
    "        :param station: Weather station that will be scraped\n",
    "        :data_type: Sets which atmospheric data will be scraped \n",
    "            (1 for temperature; 4 for wind speed)\n",
    "        \"\"\"\n",
    "        \n",
    "        table_xpath = '//*[contains(@class,\"data2_s\")]//tr'\n",
    "        elem_xpath = './/*[self::td or self::th]'\n",
    "        \n",
    "        # Finds table containing the values and selects and selects data\n",
    "        # from top to bottom and left to right\n",
    "        for tab in driver.find_elements_by_xpath(table_xpath):\n",
    "            data = [i.text for i in tab.find_elements_by_xpath(elem_xpath)]\n",
    "            \n",
    "            year = data[0]\n",
    "            \n",
    "            # Only scrapes data if the row is not the header row\n",
    "            if year != 'Year':\n",
    "                                \n",
    "                # Reshapes values array from row into column\n",
    "                values = np.array(data[1:-1])\n",
    "                values = values.reshape(-1, 1)\n",
    "                \n",
    "                # Sets timestamp for each entry to first of the month\n",
    "                dates = [pd.to_datetime(str(year)+'-'+str(m)+'-1').strftime(\n",
    "                         format='%Y-%m-%d') for m in range(1, 13, 1)]\n",
    "\n",
    "                data_insert = pd.DataFrame(data=values)\n",
    "                data_insert.columns = ['Value']\n",
    "                data_insert['Time'] = dates\n",
    "                data_insert['Station'] = station\n",
    "                \n",
    "                # Makes sure that the values represent valid floats\n",
    "                data_insert.Value = data_insert.Value.apply(lambda row: re.sub(\n",
    "                        '[^\\d+\\.+\\-]', '', row))\n",
    "                data_insert.Value = data_insert.Value.apply(lambda row: row if \n",
    "                                                        row != '' else np.nan)\n",
    "                                \n",
    "                if data_type == '1':\n",
    "\n",
    "                    self.data_temp = pd.concat([self.data_temp, data_insert],\n",
    "                                               axis=0,\n",
    "                                               ignore_index=True,\n",
    "                                               sort=False)\n",
    "\n",
    "                elif data_type == '4':\n",
    "\n",
    "                    self.data_wind = pd.concat([self.data_wind, data_insert],\n",
    "                                               axis=0,\n",
    "                                               ignore_index=True,\n",
    "                                               sort=False)\n",
    "        \n",
    "    def get_data(self):\n",
    "        \"\"\"Initializes the scraping of the stations' data. Defines with \n",
    "            stations will be scraped with their respective channel \n",
    "            (temperature or wind speed).\n",
    "        \"\"\"\n",
    "        \n",
    "        # The main page to be scraped\n",
    "        url = 'http://www.data.jma.go.jp/obd/stats/etrn/view/monthly_s3_en.php?block_no=47401&view=1'\n",
    "        \n",
    "        # Opens the URL\n",
    "        self.driver.get(url)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        \n",
    "        for station in self.station_dict:\n",
    "            \n",
    "            print('Starting job for {}'.format(station))\n",
    "            \n",
    "            # Selects the station from the drop down menu\n",
    "            sel_station = Select(self.driver.find_element_by_name('block_no'))\n",
    "            sel_station.select_by_value(station_dict[station][0])\n",
    "            \n",
    "            # Selects the data type that must be scraped for the station\n",
    "            el_xpath = '//option[@value=\"{}\"]'.format(station_dict[station][1])\n",
    "            self.driver.find_element_by_xpath(el_xpath).click()\n",
    "            \n",
    "            # Submits selection\n",
    "            sub_xpath = '//input[@value=\"Refresh\"]'\n",
    "            self.driver.find_element_by_xpath(sub_xpath).click()\n",
    "            \n",
    "            self.driver.implicitly_wait(10)\n",
    "            \n",
    "            # Starts extraction of the data\n",
    "            self.load_data(self.driver, station, station_dict[station][1])\n",
    "            \n",
    "            self.driver.implicitly_wait(10)\n",
    "            \n",
    "            print('Finished job for {}'.format(station))\n",
    "            print('\\n')\n",
    "\n",
    "    def write_data(self):\n",
    "        \"\"\"Writes the scraped data into two JSON files in the local directory.\n",
    "        \"\"\"\n",
    "            \n",
    "        self.data_temp.to_json(orient='records', path_or_buf='data_temp.json')\n",
    "        self.data_wind.to_json(orient='records', path_or_buf='data_wind.json')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job for WAKKANAI\n",
      "Finished job for WAKKANAI\n",
      "\n",
      "\n",
      "Starting job for HABORO\n",
      "Finished job for HABORO\n",
      "\n",
      "\n",
      "Starting job for RUMOI\n",
      "Finished job for RUMOI\n",
      "\n",
      "\n",
      "Starting job for OBIHIRO\n",
      "Finished job for OBIHIRO\n",
      "\n",
      "\n",
      "Starting job for OMU\n",
      "Finished job for OMU\n",
      "\n",
      "\n",
      "Starting job for SUTTSU\n",
      "Finished job for SUTTSU\n",
      "\n",
      "\n",
      "Starting job for MURORAN\n",
      "Finished job for MURORAN\n",
      "\n",
      "\n",
      "Starting job for KUTCHAN\n",
      "Finished job for KUTCHAN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The stations to be scraped, first entry is the ID used on the \n",
    "# website for each station, second entry is the ID used for data type\n",
    "# (1: temperature, 2: wind speed)\n",
    "station_dict = {\n",
    "    'WAKKANAI': ['47401', '1'],\n",
    "    'HABORO': ['47404', '1'],\n",
    "    'RUMOI': ['47406', '1'],\n",
    "    'OBIHIRO': ['47417', '1'],\n",
    "    'OMU': ['47405', '4'],\n",
    "    'SUTTSU': ['47421', '4'],\n",
    "    'MURORAN': ['47423', '4'],\n",
    "    'KUTCHAN': ['47433', '4'],\n",
    "}\n",
    "\n",
    "bot = WebScraper(station_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
